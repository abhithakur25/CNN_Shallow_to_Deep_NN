{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "#we always initialize the random number generator to a constant seed #value for reproducibility of results.\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from the path specified by the user\n",
    "def data_loader(path_train,path_test):\n",
    "   train_list=os.listdir(path_train)  \n",
    "   '''\n",
    "   # Map class names to integer labels\n",
    "   train_class_labels = { label: index for index, label in enumerate(class_names) } \n",
    "   '''\n",
    "   # Number of classes in the dataset\n",
    "   num_classes=len(train_list)\n",
    "   \n",
    "   # Empty lists for loading training and testing data images as well as corresponding labels\n",
    "   x_train=[]\n",
    "   y_train=[]\n",
    "   x_test=[]\n",
    "   y_test=[]\n",
    "   \n",
    "   # Loading training data\n",
    "   for label,elem in enumerate(train_list):\n",
    "           \n",
    "           path1=path_train+'/'+str(elem)\n",
    "           images=os.listdir(path1)\n",
    "           for elem2 in images:\n",
    "               path2=path1+'/'+str(elem2)\n",
    "               # Read the image form the directory\n",
    "               img = cv2.imread(path2)   \n",
    "               # Append image to the train data list\n",
    "               x_train.append(img)\n",
    "               # Append class-label corresponding to the image\n",
    "               y_train.append(str(label))\n",
    "   \n",
    "           # Loading testing data\n",
    "           path1=path_test+'/'+str(elem)\n",
    "           images=os.listdir(path1)\n",
    "           for elem2 in images:\n",
    "               path2=path1+'/'+str(elem2)\n",
    "               # Read the image form the directory\n",
    "               img = cv2.imread(path2)\n",
    "               # Append image to the test data list\n",
    "               x_test.append(img)\n",
    "               # Append class-label corresponding to the image\n",
    "               y_test.append(str(label))\n",
    "           \n",
    "   # Convert lists into numpy arrays\n",
    "   x_train=np.asarray(x_train)\n",
    "   y_train=np.asarray(y_train)\n",
    "   x_test=np.asarray(x_test)\n",
    "   y_test=np.asarray(y_test)\n",
    "   return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train='./Data/train'\n",
    "path_test='./Data/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 3)\n",
      "(60000,)\n",
      "(10000, 28, 28, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test=data_loader(path_train,path_test)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forcing the precision of the pixel values to be 32 bit\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs using np_utils.to_categorical inbuilt function\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the trining data into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "#The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784)\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\t#We will add 2 Convolution layers with 32 filters of 3x3, keeping the padding as same\n",
    "\tmodel.add(Conv2D(32, (3, 3), strides=(1, 1), padding = 'same' , input_shape = input_shape, activation = 'relu', kernel_initializer = 'glorot_uniform', kernel_regularizer = regularizers.l2(0.01)))\n",
    "\tmodel.add(Conv2D(32, (3, 3), strides=(1, 1), padding = 'same', activation = 'relu', kernel_initializer = 'glorot_uniform', kernel_regularizer = regularizers.l2(0.01)))\n",
    "\t#Pooling the feature map using a 2x2 pool filter\n",
    "\tmodel.add(MaxPooling2D((2, 2), strides=(2, 2), padding = 'valid'))\n",
    "\t#Adding 2 more Convolutional layers having 64 filters of 3x3\n",
    "\tmodel.add(Conv2D(64, (3, 3), strides=(1, 1), padding = 'same', activation = 'relu', kernel_initializer = 'glorot_uniform', kernel_regularizer = regularizers.l2(0.01)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), strides=(1, 1), padding = 'same', activation = 'relu', kernel_initializer = 'glorot_uniform', kernel_regularizer = regularizers.l2(0.01)))\n",
    "\t#Flatten the feature map\n",
    "\tmodel.add(Flatten())\n",
    "\t#Adding FC Layers\n",
    "\tmodel.add(Dense(500, activation='relu'))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t#A softmax activation function is used on the output\n",
    "\t#to turn the outputs into probability-like values and \n",
    "\t#allow one class of the 10 to be selected as the model's output #prediction.\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t#Checking the model summary\n",
    "\tmodel.summary()\n",
    "\t# Loading weigths\n",
    "\t#model.load_weights('./CNN.h5')\n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr = 0.001, momentum = 0.0005, decay = 0.0005)\n",
    "\tadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0005)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\t#model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               6272500   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 6,389,178\n",
      "Trainable params: 6,389,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      " - 139s - loss: 0.2271 - acc: 0.9655 - val_loss: 0.1380 - val_acc: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18c21695978>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "#The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath='./CNN.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      " - 140s - loss: 0.1429 - acc: 0.9760 - val_loss: 0.1068 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98475, saving model to ./CNN.h5\n",
      "Epoch 2/2\n",
      " - 138s - loss: 0.1155 - acc: 0.9797 - val_loss: 0.1008 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.98475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18c21695fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=200, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save_weights('./CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 1.61%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
